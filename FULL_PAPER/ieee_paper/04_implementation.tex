
This chapter details the practical realization of the \texttt{move\_car} Advanced Driver Assistance System (ADAS) 
stack, outlining the computational environment, the specific implementation of its core modules, and the integration 
strategies employed to achieve real-time autonomous navigation.

\section*{Hardware and Software Environment}

The \texttt{move\_car} system was developed and rigorously evaluated on a high-performance computing platform. 
This setup comprises an Intel® Core™ i9-14900K processor, complemented by 64GB of RAM and an NVIDIA RTX 3060 GPU 
with 12GB of dedicated memory. The operating system utilized is Ubuntu 22.04. The Robot Operating System 2 (ROS2) 
serves as the foundational software framework, chosen for its inherent modularity, robust communication capabilities, 
and real-time performance attributes crucial for inter-module data exchange. NVIDIA's TensorRT library is extensively
 integrated throughout the pipeline to optimize inference speeds.

\section*{Perception Module Implementation}

The perception module forms the cornerstone of the \texttt{move\_car} system, employing \texttt{CUDA-BEVFusion} 
for robust multi-modal data fusion from LiDAR and six surrounding cameras.

\begin{itemize}
    \item \textbf{Sensor Synchronization:} Achieving accurate multi-modal fusion necessitates precise alignment 
    of sensor inputs. LiDAR point clouds and multi-camera images are synchronized using ROS2 message filters, 
    mitigating temporal discrepancies between data captures and ensuring a coherent environmental snapshot for 
    processing.

    \item \textbf{Preprocessing:} Raw LiDAR point clouds are converted to the \texttt{pcl::PointCloud<PointXYZI>} 
    format, enabling efficient manipulation and feature extraction. 
    Concurrently, camera images undergo JPEG compression via OpenCV encoding. 
    This approach minimizes data transfer overhead within the pipeline while preserving sufficient visual information 
    for subsequent inference tasks.

    \item \textbf{Inference Acceleration:} The \texttt{CUDA-BEVFusion} model is deployed with comprehensive 
    TensorRT optimization. This integration is pivotal in accelerating inference, allowing the perception module to 
    consistently operate at approximately 25 frames per second (FPS) with an average latency of around 40 ms. 
    Such real-time performance is indispensable for reliable operation in dynamic urban driving scenarios. 
    Detected objects are transformed into a unified global coordinate 
    frame using \texttt{Eigen::Quaternion} and \texttt{Eigen::Translation3f},
     ensuring consistent spatial understanding across all modules.
\end{itemize}

\begin{table}[!t]
\centering
\caption{Inference Speed and Accuracy on KITTI Dataset}
\label{tab:inference_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model} & \textbf{Inference Speed (FP16)} & \textbf{Detection Accuracy (Car@R11)} \\
\midrule
PointPillars & 6.84 ms & 77.00\% \\
CenterPoint & \textit{Not specified} & \textit{Not specified} \\
BEVFusion & \textit{Not specified} & \textit{Not specified} \\
\bottomrule
\end{tabular}
\end{table}

\noindent *(Note: While initial evaluations on the KITTI dataset yielded specific metrics for PointPillars, 
the performance figures for CenterPoint and BEVFusion are indicated as 'Not specified' within this section, 
pending more detailed and comparative benchmarking specifically conducted for this system's configuration. 
Further comprehensive comparisons are elaborated in the dedicated Results chapter.)*

\section*{Environmental Mapping Implementation}

Beyond object detection, the \texttt{move\_car} system incorporates an environmental mapping module to construct 
detailed,real-time representations of its surroundings.
Drawing inspiration from and integrating components 
similar to the \href{https://github.com/ANYbotics/elevation_mapping}{\texttt{ANYbotics/elevation\_mapping}} package~\cite{elevationmapping}, 
this module processes LiDAR point clouds to generate high-resolution elevation maps. 
This functionality is crucial for precise terrain understanding and robust obstacle avoidance, particularly for uneven surfaces or subtle changes in road topography that standard occupancy grids might miss. The module is implemented as a dedicated ROS2 package within the project's \texttt{ros\_ws/src} directory, ensuring seamless interoperability and real-time data flow from the LiDAR sensor, allowing for continuous updates to the dynamic environmental model.

\section*{Planning and Control Modules Integration}

While the exhaustive implementation details of the planning and control functionalities are presented in subsequent chapters, 
their seamless integration is a critical aspect of the \texttt{move\_car} architecture. 
The planning module dynamically utilizes the occupancy grid generated by the perception system to compute safe, collision-free, and efficient trajectories. 
This computed trajectory is then fed to the Model Predictive Control (MPC) module. The MPC operates in a closed-loop feedback mechanism, 
translating the planned trajectory into precise vehicle commands (e.g., steering angle, acceleration, braking) that ensure smooth, 
responsive, and safe execution in continuously changing environments. The planning module typically operates at an update rate of 20 Hz, 
while the control module demonstrates robust performance with a 99th percentile latency of 45 ms.

\section*{Integration with CARLA Simulator}

The entire \texttt{move\_car} ADAS stack is seamlessly integrated with the CARLA high-fidelity urban driving simulator via a robust ROS bridge. 
This C++-implemented perception node subscribes to LiDAR and multi-camera data streams provided by the CARLA-ROS bridge, with built-in time synchronization. The raw sensor inputs are converted into GPU-compatible formats suitable for \texttt{BEVDet} (a component of the BEVFusion framework) inference. Subsequently, the derived 3D object detections are published to other planning and control modules within the ROS ecosystem. This comprehensive closed-loop simulation environment facilitated by CARLA enables extensive testing and validation of the system's real-time performance, reliability, and robustness across a diverse array of driving scenarios.

\section*{ROS2 Workspace Structure and Data Flow}

The modularity of \texttt{move\_car} is fundamentally supported by its ROS2 workspace structure, mirroring the typical organization found in autonomous driving projects. 
Within the main \texttt{ros\_ws/src} directory, separate packages are maintained for each core functional block:
\begin{itemize}
    \item \textbf{\texttt{move\_car\_perception}:} This package encapsulates the \texttt{CUDA-BEVFusion} inference engine and the sensor synchronization logic. 
    It subscribes to raw sensor topics (e.g., \texttt{/carla/hero/lidar} and \texttt{/carla/hero/camera\_*} for image streams) and publishes processed 3D object detections (e.g., \texttt{/move\_car/detections}) and dynamic occupancy grid maps (\texttt{/move\_car/occupancy\_grid}).
    \item \textbf{\texttt{move\_car\_planning}:} This package consumes the \texttt{/move\_car/detections} and \texttt{/move\_car/occupancy\_grid} topics. It implements the hierarchical planning logic, generating local trajectories (\texttt{/move\_car/local\_trajectory}) and global path updates (\texttt{/move\_car/global\_path}).
    \item \textbf{\texttt{move\_car\_control}:} This package subscribes to \texttt{/move\_car/local\_trajectory} and vehicle state information (e.g., \texttt{/carla/hero/vehicle\_status}, \texttt{/carla/hero/odometry}). It computes the necessary vehicle commands (e.g., steering angle, acceleration/braking) and publishes them to the CARLA simulator via the ROS bridge (\texttt{/carla/hero/vehicle\_control\_cmd}).
    \item \textbf{\texttt{move\_car\_utils}:} A utility package containing common data structures, coordinate transformation functions (\texttt{Eigen::Quaternion}, \texttt{Eigen::Translation3f}), and helper nodes for logging and visualization within RViz.
\end{itemize}
This distributed architecture, facilitated by ROS2's publish-subscribe mechanism, ensures that each module can be developed, tested, and optimized independently, significantly accelerating the iterative development cycle.

\section*{Dataset Integration and Model Training}

While the primary focus of this paper is on real-time inference and system integration, the perception models (PointPillars, CenterPoint, BEVFusion) underpinning \texttt{move\_car} were pre-trained on large-scale autonomous driving datasets.
\begin{itemize}
    \item \textbf{KITTI Dataset:} Utilized for 3D object detection benchmarks, providing a diverse set of real-world driving scenarios for training and evaluation of LiDAR-centric models.
    \item \textbf{nuScenes Dataset:} A comprehensive multi-modal dataset that supported the training of models requiring both LiDAR and camera inputs, enabling the development of robust fusion techniques like \texttt{CUDA-BEVFusion}.
\end{itemize}
Custom ROS2 data loaders were developed within the \texttt{ros\_ws/src} to efficiently stream data from these datasets in a format compatible with the perception pipeline, enabling seamless transition from training to simulation-based validation. The training environment leverages standard deep learning frameworks (e.g., PyTorch) with NVIDIA's CUDA toolkit for GPU acceleration.